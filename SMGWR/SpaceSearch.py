from SMGWR.Cache import Cache
import ConfigSpace as CS
import ConfigSpace.hyperparameters as CSH
from bayes_opt import BayesianOptimization
import numpy as np
import math
import random
import utils

INF = 10000000
gr = (math.sqrt(5) + 1) / 2
Min = 5
Max = 300
fib_series = [5, 8]
while fib_series[-1] < 5000:
    fib_series.append(fib_series[-1]+fib_series[-2])
fib_index = {}
for i in range(len(fib_series)):
    fib_index[fib_series[i]] = i


def get_configspace(bandwidths, n):
    """
    creates the bandwidth search space
    for our experiments, we limited the number of neighbor points between 5 and 100.
        - It can be changed to use n for the upper or lower limits
    :param bandwidths: number of bandwidths
    :param n: number of examples
    :return: returns the search space
    """
    cs = CS.ConfigurationSpace()
    bandwidth_spaces = []
    for i in range(bandwidths):
        bandwidth_spaces.append(
            CSH.UniformIntegerHyperparameter('bandwidth' + str(i), lower=Min, upper=min(n-1, Max), default_value=30, log=False))

    cs.add_hyperparameters(bandwidth_spaces)
    return cs


def generate_state(old_state, n, method, config={}):
    """
    generates a new state based on the old state and the method
    :param old_state: the  values of the previous state
    :param n: number of training points to consider
    :param method: the method to generate new state
    :param config: configurations variables
    :return: returns the new state
    """
    old_state = np.copy(np.asarray(old_state))
    if method == "gaussian_all":
        # generateas a state where each directions is a random number generated from a gaussian distributuion
        mu, sigma = 0, 0.2
        s = np.random.normal(mu, sigma, len(old_state))
        directions = s * old_state
        directions += (s / abs(s)) / 2
        directions = np.round(directions)
        res = old_state + directions
        for i in range(len(res)):
            if res[i] < Min:
                res[i] = Min
            elif res[i] > min(Max, n-1):
                res[i] = min(Max, n-1)
    elif method == "gaussian_same_all":
        # move all directions same amount generated by a gaussian distribution.
        mu, sigma = 0, 0.2
        s = np.random.normal(mu, sigma)
        directions = np.asarray([s * np.mean(old_state) for _ in range(len(old_state))])
        directions += (s / abs(s)) / 2
        directions = np.round(directions)
        res = np.asarray(old_state) + directions
        for i in range(len(res)):
            if res[i] < Min:
                res[i] = Min
            elif res[i] > min(Max, n-1):
                res[i] = min(Max, n-1)
    elif method == "gaussian_one":
        # move one of the values in old state based on gaussian distribution
        mu, sigma = 0, 0.1
        s = np.abs(np.random.normal(mu, sigma)) * 3 * old_state[config["optimizing_feature"]]
        s = np.ceil(s)
        res = np.asarray(old_state)
        res[config["optimizing_feature"]] += config["direction"] * s
        for i in range(len(res)):
            if res[i] < Min:
                res[i] = Min
            elif res[i] > min(Max, n-1):
                res[i] = min(Max, n-1)

    elif method == "fibonacci_all":
        # the new state values are generated from fibonacci values around the old values.
        mu, sigma = 0, 1
        s = np.random.normal(mu, sigma, len(old_state))
        s = np.ceil(s)
        res = old_state
        for i in range(len(old_state)):
            if old_state[i] in fib_index:
                indx = fib_index[old_state[i]]
            else:
                indx = len(fib_series)-1
                for j in range(len(fib_series)):
                    if fib_series[j] > old_state[i]:
                        indx = j
                        break
            indx = min(len(fib_series)-1, indx+int(s[i]))
            res[i] = fib_series[indx]
            if res[i] < Min:
                res[i] = Min
            elif res[i] > n-1:
                res[i] = n-1

    elif method == "fibonacci_same_all":
        # all values of old state are changed changed to the same fibonacci value around the old value.
        mu, sigma = 0, 1
        s = np.random.normal(mu, sigma)
        s = np.ceil(s)
        res = old_state
        for i in range(len(res)):
            if old_state[i] in fib_index:
                indx = fib_index[old_state[i]]
            else:
                indx = len(fib_series) - 1
                for j in range(len(fib_series)):
                    if fib_series[j] > old_state[i]:
                        indx = j
                        break
            indx = min(len(fib_series) - 1, indx + int(s))
            res[i] = fib_series[indx]
            if res[i] < Min:
                res[i] = Min
            elif res[i] > n - 1:
                res[i] = n - 1

    elif method == "fibonacci_one":
        # one variable in the old state  is changed to a fibonacci value around the old value.
        mu, sigma = 0, 1
        s = np.random.normal(mu, sigma)
        s = config["direction"] * np.abs(np.ceil(s))
        res = old_state
        i = config["optimizing_feature"]
        if old_state[i] in fib_index:
            indx = fib_index[old_state[i]]
        else:
            indx = len(fib_series) - 1
            for j in range(len(fib_series)):
                if fib_series[j] > old_state[i]:
                    indx = j
                    break

        indx = min(len(fib_series) - 1, indx + int(s))
        res[config["optimizing_feature"]] = fib_series[indx]
        if res[config["optimizing_feature"]] < Min:
            res[config["optimizing_feature"]] = Min
        elif res[config["optimizing_feature"]] > n - 1:
            res[config["optimizing_feature"]] = n - 1

    else:
        print("Wrong method name")

    return res


class SpaceSearch:
    def __init__(self, smgwr, cv=1):
        """
        :param smgwr: smgwr model passed to space search
        :param cv: if equal 1 means no cross validation and 20% of data will be considered as validation otherwise
            - this variable indicates the number of folds
        """
        self.smgwr = smgwr
        self.cs = get_configspace(smgwr.numberOfFeatures, smgwr.train_len)
        indices = np.random.permutation(smgwr.train_len)
        self.validation_indices = [[] for i in range(cv)]
        self.train_indices = [[] for i in range(cv)]
        if cv == 1:
            validationLen = int(0.14 * smgwr.train_len)
            self.validation_indices[0] = indices[:validationLen]
            self.train_indices[0] = indices[validationLen:]
        else:
            validationLen = int(smgwr.train_len / cv)
            for i in range(cv):
                self.validation_indices[i] = indices[i * validationLen:(i + 1) * validationLen]
                self.train_indices[i] = np.concatenate(
                    (indices[0:i * validationLen], indices[(i + 1) * validationLen:]))

    def hyperband(self, start_configs_count, min_budget, eta):
        """
        performs the hyperband algorithm
        :param start_configs_count: number of initial configurations
        :param min_budget: minimum budget
        :param eta: the eta variable
        :return: returns the best set of bandwidths
        """
        start_configs_count = int(start_configs_count)
        max_budget = len(self.train_indices[0])
        smax = math.ceil(math.log(start_configs_count, eta))
        best_error = INF
        for i in range(smax, -1, -1):
            n = math.ceil((smax * (eta ** i)) / max(i, 1))
            config, error = self.successive_halving(n, min(max_budget, min_budget * (eta ** (smax - i))), max_budget,
                                                    eta)
            if error < best_error:
                best_error = error
                best_config = list(config.values())
            if min_budget * (eta ** (smax - i)) > max_budget:
                break
        return best_config

    def successive_halving(self, start_configs_count, min_budget, max_budget, eta):
        """
        performs the successive halving algorithm (it is also one of the inner steps of the Hyperband)
        :return: returns the best configuration found by successive halving
        """
        start_configs_count = int(start_configs_count)
        if max_budget == -1:
            max_budget = len(self.train_indices[0])
        n = start_configs_count
        configs = [(self.cs.sample_configuration().get_dictionary(), 0) for _ in range(n)]
        budget = min_budget
        while n > 0 and budget <= max_budget:
            for i in range(len(configs)):
                configs[i] = (configs[i][0], self.evaluate(list(configs[i][0].values()), None, budget))
            configs.sort(key=lambda tup: tup[1])
            if n == 1 or budget == max_budget:
                break
            n = math.ceil(n / eta)
            configs = configs[0:n]
            if budget == max_budget:
                break
            else:
                budget = min(budget * eta, max_budget)
                if self.smgwr.config["enable_cache"]:
                    self.smgwr.cache.clear()
        return configs[0]

    def simulated_annealing(self, starting, config):
        """
        performs the simulated annealing algorithm
        :param starting: the starting set of bandwidths
        :param config: the simulated annealing configurations (number of states and updates)
        :return: returns the best configuration found by simulated annealing
        """
        config["steps"] = int(config["steps"])
        config["updates"] = int(config["updates"])

        state = np.copy(np.asarray(starting))
        best_error = self.evaluate(state)

        overall_best_error = best_error
        overall_best_state = np.copy(np.asarray(state))
        step = 0
        updates = 0
        T = 3
        alpha = 0.97

        while step < config["steps"] and updates < config["updates"]:
            new_state = generate_state(state, self.smgwr.train_len, config["method"])
            temp_error = self.evaluate(new_state, best_error)
            if temp_error < best_error:
                state = np.copy(np.asarray(new_state))
                best_error = temp_error
                updates += 1
                if best_error < overall_best_error:
                    overall_best_error = best_error
                    overall_best_state = np.copy(np.asarray(state))
            elif temp_error < 1:
                delta = (temp_error - best_error)
                r = np.random.random_sample()
                if r < np.exp(-delta * 330 / T):
                    state = np.copy(np.asarray(new_state))
                    best_error = temp_error
                    updates += 1
            step += 1
            T = max(1, T * alpha)

        return overall_best_state

    def hill_climbing(self, starting, config):
        """
        performs the hill climbing algorithm
        :param starting: the starting set of bandwidths
        :param config: the hill climbing configurations (number of states and updates)
        :return: returns the best configuration found by hill climbing
        """

        config["steps"] = int(config["steps"])
        config["updates"] = int(config["updates"])

        state = np.copy(np.asarray(starting))
        step = 0
        updates = 0
        best_error = self.evaluate(state)
        optimizing_feature = 0
        direction = 1
        while step < config["steps"] and updates < config["updates"]:
            new_state = generate_state(state, self.smgwr.train_len, config["method"],
                                       {"optimizing_feature": optimizing_feature, "direction": direction})
            step += 1
            temp_error = self.evaluate(new_state, best_error)
            if temp_error < best_error:
                state = np.copy(np.asarray(new_state))
                best_error = temp_error
                updates += 1
            else:
                direction = -1 * direction
                new_state = generate_state(state, self.smgwr.train_len, config["method"],
                                           {"optimizing_feature": optimizing_feature, "direction": direction})
                step += 1
                temp_error = self.evaluate(new_state, best_error)
                if temp_error < best_error:
                    state = np.copy(np.asarray(new_state))
                    best_error = temp_error
                    updates += 1
                else:
                    optimizing_feature += 1
                    direction = 1
                    optimizing_feature %= self.smgwr.numberOfFeatures

        return state

    def thorough_search(self, starting, config):
        """
        checks all the possible combination of bandwdiths where the values can be Max or Min
        checks 2^m configurations where m is the number of features
        :return: returns the best set of bandwidths
        """
        best_error = self.evaluate(starting)
        state = np.copy(np.asarray(starting))
        for i in range(2 ** self.smgwr.numberOfFeatures):
            new_state = [Max if i & (1 << j) else Min for j in range(self.smgwr.numberOfFeatures)]
            temp_error = self.evaluate(new_state, best_error)
            if temp_error < best_error:
                state = np.copy(np.asarray(new_state))
                best_error = temp_error

        return state

    def golden_section(self, starting, config={"steps": 100}):
        """
        performs the golden section search
        :return: returns the best configuration found by golden section search
        """
        config["steps"] = int(config["steps"])

        step = 0
        best_error = self.evaluate(starting)
        state = np.copy(np.asarray(starting))
        best_state = np.copy(np.asarray(starting))
        while step < config["steps"]:
            for optimizing_feature in range(self.smgwr.numberOfFeatures):
                a = Min
                b = Max
                c = b - (b - a) / gr
                d = a + (b - a) / gr
                while abs(b - a) > 1 and step < config["steps"]:
                    state[optimizing_feature] = c
                    eval_c = self.evaluate(state)
                    state[optimizing_feature] = d
                    eval_d = self.evaluate(state)
                    step += 2
                    if eval_c < eval_d:
                        b = d
                    else:
                        a = c

                    c = b - (b - a) / gr
                    d = a + (b - a) / gr
                state[optimizing_feature] = round((b + a) / 2)
                temp_error = self.evaluate(state)
                if temp_error < best_error:
                    best_error = temp_error
                    best_state = np.copy(np.asarray(state))

        return best_state

    def bayesian_optimization(self, starting, config={}):
        """
        performs the bayesia optimization algorithm
        :param config: contains the configurations such as the number of iteration or the locality of old state
            -in which the algorithm looks for the best set of bandwdiths
        :return: returns the best set of bandwidths found by bayesian optimization method
        """
        default_config = {"is_local": False, "locality_range": 10, "random_count": 50, "iter_count": 60}
        for key in config.keys():
            default_config[key] = config[key]
        config = default_config

        config["random_count"] = int(config["random_count"])
        config["iter_count"] = int(config["iter_count"])

        pbounds = {}
        if config["is_local"]:
            for _ in range(self.smgwr.numberOfFeatures):
                temp_min = max(Min, starting[_]-config["locality_range"])
                temp_max = min(Max, self.smgwr.train_len-1, starting[_]+config["locality_range"])
                if temp_min > temp_max:
                    temp_min, temp_max = temp_max, temp_min
                pbounds['bandwidth' + str(_)] = (temp_min, temp_max)
        else:
            for _ in range(self.smgwr.numberOfFeatures):
                temp_min = Min
                temp_max = min(Max, self.smgwr.train_len-1)
                if temp_min > temp_max:
                    temp_min, temp_max = temp_max, temp_min
                pbounds['bandwidth' + str(_)] = (temp_min, temp_max)

        optimizer = BayesianOptimization(
            f=self.bayesian_optimization_evaluate,
            pbounds=pbounds,
            random_state=4,
            verbose=0
        )

        # print(starting, config["random_count"], config["iter_count"])
        optimizer.probe(starting, lazy=True)
        optimizer.maximize(
            init_points=config["random_count"],
            n_iter=config["iter_count"],
        )
        bandwidths = list(optimizer.max["params"].values())
        bandwidths = np.asarray(bandwidths).astype(int)
        return bandwidths

    def FDSA(self, starting, config= {}):
        default_config = {"alpha_decay": 0.95, "gamma_decay": 0.95, "steps": 50}
        for key in config.keys():
            default_config[key] = config[key]
        config = default_config
        config["steps"] = int(config["steps"])

        best_state = np.copy(np.asarray(starting))
        state = np.copy(np.asarray(starting))
        best_error = self.evaluate(starting)

        grad = np.zeros(len(state)) + best_error
        for iter in range(config["steps"]):
            ck = (1 / 10) * (config["gamma_decay"] ** (iter + 1))
            ak = (4 / grad) * (config["alpha_decay"] ** (iter + 1))
            for feature in range(len(state)):
                bw = np.copy(state)
                delta = np.zeros(len(state))
                delta[feature] = 1
                ta = state + (ck * delta)
                tb = state - (ck * delta)

                ta[feature] = round(ta[feature])
                ta[feature] = max(Min, ta[feature])
                ta[feature] = min(ta[feature], Max, self.smgwr.train_len - 1)

                tb[feature] = round(tb[feature])
                tb[feature] = max(Min, tb[feature])
                tb[feature] = min(tb[feature], Max, self.smgwr.train_len - 1)

                if ta[feature] == tb[feature]:
                    ta[feature] += 1

                ya = self.evaluate(ta)
                yb = self.evaluate(tb)
                grad[feature] = (ya - yb) / (ta[feature] - tb[feature])

            temp = ak * grad
            for j in range(len(temp)):
                if temp[j] < 0:
                    temp[j] = math.floor(temp[j])
                else:
                    temp[j] = math.ceil(temp[j])
            state = state - temp
            for j in range(len(ta)):
                state[j] = max(Min, state[j])
                state[j] = min(state[j], Max, self.smgwr.train_len-1)
            temp_error = self.evaluate(state)
            if temp_error < best_error:
                best_error = temp_error
                best_state = np.copy(np.asarray(state))

        return best_state

    def SPSA(self, starting, config= {}):
        """
        performs the SPSA algorithm
        """
        default_config = {"alpha_decay": 0.93, "gamma_decay": 0.95, "steps": 50}
        for key in config.keys():
            default_config[key] = config[key]
        config = default_config
        config["steps"] = int(config["steps"])

        best_state = np.copy(np.asarray(starting))
        state = np.copy(np.asarray(starting))
        best_error = self.evaluate(starting)


        base = None
        for iter in range(config["steps"]):
            ck = (1/10)*(config["gamma_decay"]**(iter+1))
            if base is None:
                ak = (1/max(best_error, 0.00001))*(config["alpha_decay"]**(iter+1))
            else:
                ak = (1 / max(base, 0.00001)) * (config["alpha_decay"] ** (iter + 1))
            delta = np.asarray([random.choice((-state[_], state[_])) for _ in range(len(state))])
            ta = state + (ck * delta)
            tb = state - (ck * delta)
            for j in range(len(ta)):
                ta[j] = round(ta[j])
                ta[j] = max(Min, ta[j])
                ta[j] = min(ta[j], Max, self.smgwr.train_len-1)

                tb[j] = round(tb[j])
                tb[j] = max(Min, tb[j])
                tb[j] = min(tb[j], Max, self.smgwr.train_len-1)

                if ta[j] == tb[j]:
                    ta[j] -= 1

            ya = self.evaluate(ta)
            yb = self.evaluate(tb)
            gk = (ya-yb) / (ta-tb)
            if base is None:
                base = max(gk)
            temp = (ak * gk) * state
            for j in range(len(temp)):
                if temp[j] < 0:
                    temp[j] = math.floor(temp[j])
                else:
                    temp[j] = math.ceil(temp[j])
            state = state - temp
            for j in range(len(ta)):
                state[j] = max(Min, state[j])
                state[j] = min(state[j], Max, self.smgwr.train_len-1)
            temp_error = self.evaluate(state)
            if temp_error < best_error:
                best_error = temp_error
                best_state = np.copy(np.asarray(state))

        return best_state


    def bayesian_optimization_evaluate(self, **bandwidths):
        state = list(bandwidths.values())
        errors = np.zeros(len(self.validation_indices))
        for i in range(len(self.validation_indices)):
            self.smgwr.fit(state, self.train_indices[i])
            valid_pred = self.smgwr.validate(state, self.validation_indices[i], self.train_indices[i][0:-1])
            errors[i] = utils.R2(self.smgwr.y_training[self.validation_indices[i]], valid_pred)

        error = np.mean(errors)
        return 1-error

    def evaluate(self, state, best_so_far=None, budget=-1):
        errors = np.zeros(len(self.validation_indices))
        for i in range(len(self.validation_indices)):
            if budget == -1:
                self.smgwr.fit(state, self.train_indices[i])
            else:
                self.smgwr.fit(state, self.train_indices[i][0:budget])
            valid_pred = self.smgwr.validate(state, self.validation_indices[i], self.train_indices[i][0:budget])
            errors[i] = utils.R2(self.smgwr.y_training[self.validation_indices[i]], valid_pred)

        error = np.mean(errors)
        return error
